genera una nueva versión de este código haciendo posible que se interrumpa la reproducción del audio si el usuario dice algo, y usando esa nueva frase del usuario para obtener la siguiente respuesta de gpt-4o-mini, teniendo mucho cuidado de no retroalimentar el micrófono con la reproducción del audio:


genera una nueva versión de este código añadiéndole un interfaz web con un botón para empezar a escuchar al usuario y haciendo posible que se interrumpa la reproducción del audio si el usuario dice algo, y usando esa nueva frase del usuario para obtener la siguiente respuesta de gpt-4o-mini, teniendo mucho cuidado de no retroalimentar el micrófono con la reproducción del audio:

"'''
Speech-to-GPT-4o-mini-to-TTS con streaming
transcripción en tiempo real  
respuesta de GPT-4o-mini en streaming  
TTS en streaming y reproducción instantánea  
'''

import os  
import json  
import base64  
import threading  
import queue  
import re  
import time  
  
import websocket  
import pyaudio  
import requests                          # (lo usa websocket-client)  
from dotenv import load_dotenv  
from openai import AzureOpenAI  
  
# ---------------------------------------------------------------------------  
# Carga de variables de entorno  
# ---------------------------------------------------------------------------  
load_dotenv(override=True)  
  
# ---------------------------------------------------------------------------  
# Constantes de audio  
# ---------------------------------------------------------------------------  
RATE            = 24_000                 # 24 kHz → coincide con las voces de Azure  
CHANNELS        = 1  
FORMAT          = pyaudio.paInt16        # 16-bit little-endian  
CHUNK           = 1024  
WAV_HEADER_LEN  = 44                     # bytes de cabecera RIFF/WAV  

PROMPT_STT = "Your response **MUST** be in the same language than the user's question."
SYSTEM_PROMPT_CHAT = "You are a helpful assistant. Respond in the same language than the user's question." # Respond in Spanish.

is_playing_audio = threading.Event()     # se activa mientras suena el TTS  
  
# ---------------------------------------------------------------------------  
# Websocket STT (transcripción)  
# ---------------------------------------------------------------------------  
ws_url = (  
    f'{os.environ["AZURE_OPENAI_ENDPOINT_STT"].replace("https", "wss")}'  
    f'/openai/realtime?api-version={os.environ["AZURE_OPENAI_API_VERSION_STT"]}&intent=transcription'  
)  
ws_headers = {"api-key": os.environ["AZURE_OPENAI_API_KEY_STT"]}  
  
# ---------------------------------------------------------------------------  
# PyAudio – micrófono y altavoz  
# ---------------------------------------------------------------------------  
audio = pyaudio.PyAudio()  
  
mic_stream = audio.open(  
    format=FORMAT,  
    channels=CHANNELS,  
    rate=RATE,  
    input=True,  
    frames_per_buffer=CHUNK,  
)  
  
# Salida de altavoz reutilizable (evita chasquidos al abrir/cerrar)  
speaker_out = audio.open(  
    format=FORMAT,  
    channels=CHANNELS,  
    rate=RATE,  
    output=True,  
    frames_per_buffer=CHUNK,  
)  
  
# ---------------------------------------------------------------------------  
# Clientes Azure OpenAI  
# ---------------------------------------------------------------------------  
aoai_client = AzureOpenAI(  
    azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],  
    api_key=os.environ["AZURE_OPENAI_API_KEY"],  
    api_version=os.environ["AZURE_OPENAI_API_VERSION"],  
)  
AOAI_DEPLOYMENT_NAME = os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"]  
AOAI_DEPLOYMENT_NAME_STT = os.environ["AZURE_OPENAI_DEPLOYMENT_NAME_STT"]

tts_client = AzureOpenAI(  
    azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT_TTS"],  
    api_key=os.environ["AZURE_OPENAI_API_KEY_TTS"],  
    api_version=os.environ["AZURE_OPENAI_API_VERSION_TTS"],  
)  
  

  
# ---------------------------------------------------------------------------  
# Utils  
# ---------------------------------------------------------------------------  
def play_audio_streaming(pcm_iter):  
    '''  
    Reproduce el audio recibido en streaming:
    1. Descarta la cabecera WAV que envía TTS.  
    2. Garantiza que siempre se escriben pares de bytes (16-bit).  
    3. Usa un único stream de salida para evitar chasquidos.  
    '''  
    first_chunk = True  
    leftover    = b""               # byte suelto que quede entre llamadas  
  
    for chunk in pcm_iter:  
        if not chunk:  
            continue  
  
        # --- 1) Quitar cabecera WAV la primera vez -------------------  
        if first_chunk:  
            first_chunk = False  
            if chunk.startswith(b"RIFF"):  
                chunk = chunk[WAV_HEADER_LEN:]   # saltamos cabecera  
                if not chunk:  
                    continue  
  
        # --- 2) Alinear a 16-bit (multiplo de 2 bytes) ---------------  
        chunk = leftover + chunk  
        if len(chunk) & 1:                       # tamaño impar  
            leftover, chunk = chunk[-1:], chunk[:-1]  
        else:  
            leftover = b""  
  
        # --- 3) Reproducción -----------------------------------------  
        if chunk:  
            speaker_out.write(chunk)  
  
    # Si sobró un byte lo descartamos; no se puede reproducir media muestra.  
  
# ----------------------------------------------------------------------------  
# GPT-4o-mini + TTS – todo en streaming  
# ----------------------------------------------------------------------------  
def assistant_stream(question: str):  
    '''  
    Obtiene la respuesta de GPT-4o-mini en streaming  
    y la va enviando por frases a TTS.  
    '''  
    # Cola texto→TTS  
    tts_queue: queue.Queue[str | None] = queue.Queue()  
  
    # --- worker que consume la cola y reproduce cada fragmento ----------  
    def tts_worker():  
        is_playing_audio.set()                      # pausa el envío del micro  
        #print("\nAssistant:", end=" ", flush=True)
        while True:  
            fragment = tts_queue.get() 
            #print(fragment, end=" ", flush=True)
            if fragment is None or fragment == "":  
                break  
  
            # TTS en streaming  
            with tts_client.audio.speech.with_streaming_response.create(  
                model="gpt-4o-mini-tts",  
                voice="ballad",                    # voz  
                input=fragment,  
                instructions=(  
                    "Affect/personality: A cheerful guide\n\n"  
                    "Tone: Friendly, clear, and reassuring.\n"  
                    "Pause: Brief pauses after key instructions.\n"  
                    "Emotion: Warm and supportive."  
                ),  
                response_format="pcm",  
            ) as tts_response:  
                play_audio_streaming(tts_response.iter_bytes())  

        print('\n____________________________________________________')
        print("¡Dime algo más!")
        is_playing_audio.clear()                   # reanuda el micro  
    
    threading.Thread(target=tts_worker, daemon=True).start()  
  
    # ---  Solicitamos chat en streaming ---------------------------------  
    buffer = ""  
    print("\nAssistant:\n", end=" ", flush=True)  
  
    for chunk in aoai_client.chat.completions.create(  
        model=AOAI_DEPLOYMENT_NAME,  
        messages=[  
            {"role": "system", "content": SYSTEM_PROMPT_CHAT},
            {"role": "user",   "content": question},  
        ],  
        temperature=0.7,  
        max_tokens=1000,  
        stream=True,  
    ):  
        if not chunk.choices:  
            continue  
  
        for choice in chunk.choices:  
            delta_obj     = getattr(choice, "delta", None)  
            content_piece = getattr(delta_obj, "content", None)  
            if not content_piece:  
                continue  
  
            # 1) Mostrar por pantalla  
            print(content_piece, end="", flush=True)  
  
            # 2) Acumular y trocear por final de frase  
            buffer += content_piece  
            if re.search(r"[.!?\n]$", buffer):  
                tts_queue.put(buffer.strip())  
                buffer = ""  
  
    # Cualquier texto restante  
    if buffer.strip():  
        tts_queue.put(buffer.strip())  
  
    # Señal de fin al worker de TTS  
    tts_queue.put(None)  
  
# ---------------------------------------------------------------------------  
# Callbacks websocket STT  
# ---------------------------------------------------------------------------  
def on_open(ws):  
    print("Conectado. ¡Habla!")  
  
    session_cfg = {  
        "type": "transcription_session.update",  
        "session": {  
            "input_audio_format": "pcm16",  
            "input_audio_transcription": {  
                "model": AOAI_DEPLOYMENT_NAME_STT, #"gpt-4o-mini-transcribe",  
                "prompt": PROMPT_STT,  
            },  
            "input_audio_noise_reduction": {"type": "near_field"},  
            "turn_detection": {"type": "server_vad"},  
        },  
    }  
    ws.send(json.dumps(session_cfg))  
  
    # Hilo que envía audio del micrófono  
    def mic_sender():  
        try:  
            while ws.keep_running:  
                if is_playing_audio.is_set():      # si suena TTS: pausa micro  
                    time.sleep(0.05)  
                    continue  
                data = mic_stream.read(CHUNK, exception_on_overflow=False)  
                ws.send(  
                    json.dumps(  
                        {  
                            "type": "input_audio_buffer.append",  
                            "audio": base64.b64encode(data).decode(),  
                        }  
                    )  
                )  
        except Exception as exc:  
            print("Error enviando audio:", exc)  
            ws.close()  
  
    threading.Thread(target=mic_sender, daemon=True).start()  
  
def on_message(ws, message):  
    try:  
        ev = json.loads(message)  
        etype = ev.get("type", "")  
  
        if etype == "conversation.item.input_audio_transcription.delta":  
            print(ev.get("delta", ""), end=" ", flush=True)  
  
        elif etype == "conversation.item.input_audio_transcription.completed":  
            transcript = ev["transcript"]  
            print(f"\n>> {transcript}\n")  
  
            # Lanza GPT-4o-mini + TTS en un hilo para no bloquear el websocket  
            threading.Thread(  
                target=assistant_stream, args=(transcript,), daemon=True  
            ).start()  
  
    except Exception as exc:  
        print("on_message error:", exc)  
  
def on_error(ws, error):  
    print("Websocket error:", error)  
  
def on_close(ws, code, reason):  
    print("Websocket cerrado:", code, reason)  
    mic_stream.stop_stream()  
    mic_stream.close()  
    speaker_out.stop_stream()  
    speaker_out.close()  
    audio.terminate()  
  
# ---------------------------------------------------------------------------  
# Lanzamos…  
# ---------------------------------------------------------------------------  
print("Conectando a:", ws_url)  
ws_app = websocket.WebSocketApp(  
    ws_url,  
    header=ws_headers,  
    on_open=on_open,  
    on_message=on_message,  
    on_error=on_error,  
    on_close=on_close,  
)  
  
ws_app.run_forever()"